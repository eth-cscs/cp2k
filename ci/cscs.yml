include:
  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'

stages:
  - build_base
  - build
  #  - test

build fedora base image:
  extends: .container-builder
  stage: build_base
  timeout: 4h
  before_script:
  - DOCKER_TAG=`sha256sum ci/baseimage.cpu.Fedora37.Dockerfile | head -c 16`
  - export PERSIST_IMAGE_NAME=$CSCS_REGISTRY_PATH/baseimage/cp2k_base:$DOCKER_TAG
  - echo "BASE_IMAGE=$PERSIST_IMAGE_NAME" >> build.env
  artifacts:
    reports:
      dotenv: build.env
  variables:
    #CSCS_BUILD_IN_MEMORY: 'FALSE'
    DOCKERFILE: ci/baseimage.cpu.Fedora37.Dockerfile
    # change to 'always' if you want to rebuild, even if target tag exists already (if-not-exists is the default, i.e. we could also skip the variable)
    CSCS_REBUILD_POLICY: if-not-exists
    #DOCKER_BUILD_ARGS: '["CUDA_ARCH=60"]'

build ubuntu base image:
  extends: .container-builder
  stage: build_base
  # we create a tag that depends on the SHA value of ci/baseimage.cuda.ubuntu.Dockerfile, this way
  # a new base image is only built when the SHA of this file changes
  # If there are more dependency files that should change the tag-name of the base container
  # image, they can be added too.
  # Since the base image name is runtime dependent, we need to carry the value of it to
  # the following jobs via a dotenv file.
  timeout: 4h
  before_script:
  - DOCKER_TAG=`sha256sum ci/baseimage.ubuntu.Dockerfile | head -c 16`
  - export PERSIST_IMAGE_NAME=$CSCS_REGISTRY_PATH/baseimage/cp2k_base:$DOCKER_TAG
  - echo "BASE_IMAGE=$PERSIST_IMAGE_NAME" >> build.env
  artifacts:
    reports:
      dotenv: build.env
  variables:
    DOCKERFILE: ci/baseimage.ubuntu.Dockerfile
    # change to 'always' if you want to rebuild, even if target tag exists already (if-not-exists is the default, i.e. we could also skip the variable)
    CSCS_REBUILD_POLICY: if-not-exists
    DOCKER_BUILD_ARGS: '[]'


build cuda base image:
  extends: .container-builder
  stage: build_base
  # we create a tag that depends on the SHA value of ci/baseimage.cuda.ubuntu.Dockerfile, this way
  # a new base image is only built when the SHA of this file changes
  # If there are more dependency files that should change the tag-name of the base container
  # image, they can be added too.
  # Since the base image name is runtime dependent, we need to carry the value of it to
  # the following jobs via a dotenv file.
  timeout: 2h
  before_script:
  - DOCKER_TAG=`sha256sum ci/baseimage.cuda.ubuntu.Dockerfile | head -c 16`
  - export PERSIST_IMAGE_NAME=$CSCS_REGISTRY_PATH/baseimage/cp2k_base:$DOCKER_TAG
  - echo "BASE_IMAGE=$PERSIST_IMAGE_NAME" >> build.env
  artifacts:
    reports:
      dotenv: build.env
  variables:
    DOCKERFILE: ci/baseimage.cuda.ubuntu.Dockerfile
    # change to 'always' if you want to rebuild, even if target tag exists already (if-not-exists is the default, i.e. we could also skip the variable)
    CSCS_REBUILD_POLICY: if-not-exists
    DOCKER_BUILD_ARGS: '["CUDA_ARCH=80"]'

build rocm base image:
  extends: .container-builder
  stage: build_base
  # we create a tag that depends on the SHA value of ci/baseimage.cuda.ubuntu.Dockerfile, this way
  # a new base image is only built when the SHA of this file changes
  # If there are more dependency files that should change the tag-name of the base container
  # image, they can be added too.
  # Since the base image name is runtime dependent, we need to carry the value of it to
  # the following jobs via a dotenv file.
  timeout: 4h
  before_script:
  - DOCKER_TAG=`sha256sum ci/baseimage.rocm.ubuntu.Dockerfile | head -c 16`
  - export PERSIST_IMAGE_NAME=$CSCS_REGISTRY_PATH/baseimage/cp2k_base:$DOCKER_TAG
  - echo "BASE_IMAGE=$PERSIST_IMAGE_NAME" >> build.env
  artifacts:
    reports:
      dotenv: build.env
  variables:
    DOCKERFILE: ci/baseimage.rocm.ubuntu.Dockerfile
    # change to 'always' if you want to rebuild, even if target tag exists already (if-not-exists is the default, i.e. we could also skip the variable)
    CSCS_REBUILD_POLICY: if-not-exists
    DOCKER_BUILD_ARGS: '["AMDGPU_TARGET=gfx90a"]'


build fedora-mpich-cpu image:
  extends: .container-builder
  needs: ["build fedora base image"]
  stage: build
variables:
  DOCKERFILE: ci/build.Dockerfile
  PERSIST_IMAGE_NAME: discard
  SPEC: 'cp2k@master%gcc +libxc +libint smm=libxsmm +spglib +mpi +openmp +cosma ^intel-oneapi-mkl+cluster ^mpich@4.0.3 ^dbcsr+mpi+openmp~shared ^cosma+scalapack+shared'
  DOCKER_BUILD_ARGS: '["BASE_IMAGE=${BASE_IMAGE}", "SPECDEV=$SPEC", "CMAKE_ARG=$CMAKE_ARG"]'

build fedora-openmpi-cpu image:
  extends: .container-builder
  needs: ["build fedora base image"]
  stage: build
  variables:
    DOCKERFILE: ci/build.Dockerfile
    PERSIST_IMAGE_NAME: discard
    SPEC: 'cp2k@master%gcc +libxc +libint smm=libxsmm +spglib +mpi +openmp +cosma ^intel-oneapi-mkl+cluster ^openmpi ^dbcsr+mpi+openmp~shared ^cosma+scalapack+shared'
    DOCKER_BUILD_ARGS: '["BASE_IMAGE=${BASE_IMAGE}", "SPECDEV=$SPEC", "CMAKE_ARG=$CMAKE_ARG"]'


build ubuntu-mpich-gpu image:
  extends: .container-builder
  needs: ["build cuda base image"]
  stage: build
  variables:
    DOCKERFILE: ci/build.Dockerfile
    PERSIST_IMAGE_NAME: discard
    SPEC: 'cp2k@master%gcc +libxc +libint smm=libxsmm +spglib +mpi +openmp +cuda cuda_arch=80 +cosma ^intel-oneapi-mkl+cluster ^mpich@4.0.3 ^cosma+scalapack+shared+cuda~apps~tests ^dbcsr+cuda~shared+mpi+openmp cuda_arch=80'
    DOCKER_BUILD_ARGS: '["BASE_IMAGE=${BASE_IMAGE}", "SPECDEV=$SPEC", "CMAKE_ARG=$CMAKE_ARG"]'

build ubuntu-mpich-rocm image:
  extends: .container-builder
  needs: ["build rocm base image"]
  stage: build
  variables:
    DOCKERFILE: ci/build.Dockerfile
    PERSIST_IMAGE_NAME: discard
    SPEC: 'cp2k@master%gcc +libxc +libint smm=libxsmm +spglib +mpi +openmp +rocm amdgpu_target=gfx90a +cosma ^intel-oneapi-mkl+cluster ^mpich@4.0.3 ^cosma+scalapack+shared+rocm~apps~tests ^dbcsr@2.6.0+rocm~shared+mpi+openmp amdgpu_target=gfx90a'
    DOCKER_BUILD_ARGS: '["BASE_IMAGE=${BASE_IMAGE}", "SPECDEV=$SPEC", "CMAKE_ARG=$CMAKE_ARG"]'

build ubuntu-mpich-cpu image:
  extends: .container-builder
  needs: ["build ubuntu base image"]
  stage: build
  variables:
    DOCKERFILE: ci/build.Dockerfile
    PERSIST_IMAGE_NAME: discard
    SPEC: 'cp2k@master%gcc +libxc +libint smm=libxsmm +spglib +mpi +openmp +cosma ^intel-oneapi-mkl+cluster ^mpich@4.0.3 ^dbcsr+mpi+openmp~shared ^cosma+scalapack+shared'
    DOCKER_BUILD_ARGS: '["BASE_IMAGE=${BASE_IMAGE}", "SPECDEV=$SPEC", "CMAKE_ARG=$CMAKE_ARG"]'

build ubuntu-openmpi-cpu image:
  extends: .container-builder
  needs: ["build ubuntu base image"]
  stage: build
  variables:
    DOCKERFILE: ci/build.Dockerfile
    PERSIST_IMAGE_NAME: discard
    SPEC: 'cp2k@master%gcc +libxc +libint smm=libxsmm +spglib +mpi +openmp +cosma ^intel-oneapi-mkl+cluster ^openmpi ^dbcsr+mpi+openmp~shared ^cosma+scalapack+shared'
    DOCKER_BUILD_ARGS: '["BASE_IMAGE=${BASE_IMAGE}", "SPECDEV=$SPEC", "CMAKE_ARG=$CMAKE_ARG"]'

    # .run_tests:
#   extends: .container-runner-daint-gpu
#   needs: ["build cuda image"]
#   stage: test
#   script:
#     - cd /sirius-src/spack-build
#     - |
#       if [ "$SLURM_PROCID" == "0" ]; then
#         $TEST_COMMAND -V
#       else
#         $TEST_COMMAND --output-on-failure
#       fi
#   image: $CSCS_REGISTRY_PATH/sirius/sirius-ci:$CI_COMMIT_SHA
#   variables:
#     CRAY_CUDA_MPS: 1
#     GIT_STRATEGY: none
#     MPICH_MAX_THREAD_SAFETY: multiple
#     CSCS_REGISTRY_LOGIN: 'YES'
#     PULL_IMAGE: 'YES'
#     SLURM_HINT: nomultithread
#     SLURM_JOB_NUM_NODES: 1
#     SLURM_UNBUFFEREDIO: ''
#     SLURM_WAIT: 0

# gpu serial:
#   extends: .run_tests
#   variables:
#     OMP_NUM_THREADS: 12
#     SLURM_CONSTRAINT: gpu
#     SLURM_CPUS_PER_TASK: 12
#     SLURM_NTASKS: 1
#     SLURM_TIMELIMIT: "30:00"
#     TEST_COMMAND: ctest -L gpu_serial

# gpu band parallel:
#   extends: .run_tests
#   variables:
#     OMP_NUM_THREADS: 3
#     SLURM_CONSTRAINT: gpu
#     SLURM_CPUS_PER_TASK: 3
#     SLURM_NTASKS: 4
#     SLURM_TIMELIMIT: "30:00"
#     TEST_COMMAND: ctest -L gpu_band_parallel
#     USE_MPI: 'YES'

# gpu k-point parallel:
#   extends: .run_tests
#   variables:
#     OMP_NUM_THREADS: 3
#     SLURM_CONSTRAINT: gpu
#     SLURM_CPUS_PER_TASK: 3
#     SLURM_NTASKS: 4
#     SLURM_TIMELIMIT: "30:00"
#     TEST_COMMAND: ctest -L gpu_k_point_parallel
#     USE_MPI: 'YES'

# cpu single:
#   extends: .run_tests
#   variables:
#     OMP_NUM_THREADS: 12
#     SLURM_CONSTRAINT: gpu
#     SLURM_CPU_BIND: sockets
#     SLURM_CPUS_PER_TASK: 12
#     SLURM_NTASKS: 1
#     SLURM_TIMELIMIT: "30:00"
#     TEST_COMMAND: ctest -L cpu_serial

# cpu band parallel:
#   extends: .run_tests
#   variables:
#     OMP_NUM_THREADS: 3
#     SLURM_CONSTRAINT: gpu
#     SLURM_CPU_BIND: sockets
#     SLURM_CPUS_PER_TASK: 3
#     SLURM_NTASKS: 4
#     SLURM_TIMELIMIT: "30:00"
#     TEST_COMMAND: ctest -L cpu_band_parallel
#     USE_MPI: 'YES'
